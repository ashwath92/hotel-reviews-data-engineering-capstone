{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Hotel reviews and geographical, demographic, economic data\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This is a data engineering (ETL) project to combine data related to hotels from disparate sources, while also adding some additional data to enable some different kinds of analyses. The data sets used (explained in detail in the next section) include hotel review data scraped from Booking.com (with some sentiment analysis features included), data from Google maps, airport data, and tourism, economic, financial and political data from UN Agencies and a few other sources. A number of analyses can be done: based on reviewers' nationalities (and some political/economic/ indicators of their nationalities), on the hotel country (and its connection with the reviewers' nationality), on the number of reviews for a hotel, number of ratings, and so on.  For this project, the tools used are Apache Spark (Pyspark), Pandas, Amazon S3, Airflow and Amazon Redshift. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up\n",
    "\n",
    "I use this notebook mostly to provide a walkthrough of the whole project, to describe the data sets and data model, and to add more information about the project.\n",
    "\n",
    "Most of the heavy lifting is done in other notebooks and in Airflow. These are the main notebooks: CleanGooglePlaces.ipynb, PrepareDataSetsForS3.ipynb (this is the most important notebook in this project) and CombineAdditionalCountryData.ipynb. There are additional notebooks for data explorations. Finally, we have Air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jsonlines\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from jsonlines) (1.11.0)\n",
      "Installing collected packages: jsonlines\n",
      "Successfully installed jsonlines-1.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "### The main goals of my project are: \n",
    "1. to get a data set of hotel reviews in Europe and supplement it with data from Google Maps (Google Local). This will create a much richer data set as it would allow us to utilise things like the working hours of the hotel.\n",
    "2. to add in information about the nearest airport to our hotels.\n",
    "3. the original reviews data set is meant for sentiment analysis. I want to enable the end user to do many more kinds of analyses. I use data from the UN and other agencies about countries (both hotels' countries and the reviewer's countries) to allow additional analyses to see if there are patterns in reviews based on reviewers' nationality and its relation to the country where the hotel is located. For this, I use tourism data, economic, political and demographic data to see if these indicators of a country's economic and general well-being has an effect on its citizens' reviews. For example, do people from countries with low GDP tend to review hotels in countires with high GDP more favourably? See the CombineAdditionalCountryData.ipynb notebook for a lot more information about the type of analyses which can be made)  \n",
    "\n",
    "4. to build a dimensional model with a central 'reviews' factless fact table and dimensions related to hotels, airports, addresses and time. An additional fact table containing various measures related to countries will also be present.\n",
    "\n",
    "### Overview of end solution and tools used\n",
    "\n",
    "I use the following tools:\n",
    "1. Apache Spark: to clean and prepare the data sets. It takes care of the Extract and Transform steps of the ETL pipeline. Spark is a good tool for this because not only does it handle large amounts of data easily, but it also allows extremely powerful data manipulation. The primary data sets of this project (reviews, Google local and Airport codes) cannot be joined directly on any field. We need a fuzzy match for the location data, which is easier to do in Spark than in SQL. The process to prepare the data is very complicated (as you can see in the PrepareDataSetsForS3.ipynb notebook), so Spark plays a massive role in this project. In summary, Spark is used to prepare clean  data sets so that they're ready to be staged in S3 and then Redshift.\n",
    "2. Pandas: the additional country data sets are wrangled, cleaned and combined using Pandas. These data sets are from the UN, IMF, UNDP, UNWTO, Freedom House and Our World in Data. I used Pandas because these data sets are quite small, and Pandas is an excellent and efficient solution when the data isn't too large.\n",
    "3. Amazon S3: all the prepared data sets are stored in S3 in Apache Parquet format. S3 can be thought of as a staging area in this project, as an intermediate location between Spark (extract, transform) and Redshift (load).\n",
    "3. Redshift: the final data warehouse is stored in Amazon Redshift. Redshift, in conjunction with Apache Airflow, takes care of the Load part of the ETL pipeline. Staging tables are created with data loaded in from S3 (data which has been prepared using Spark and Pandas), and these are used to load the final data warehouse fact and dimension tables.\n",
    "4. Airflow: Airflow is used to orchestrate the data flow after the initial data sets have been prepared and staged in S3. In this project, it is used to load data into Redshift and to run a couple of final data quality checks.\n",
    "\n",
    "### Data used\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "The data sets used are described in some detail below. I have included a table with the number of rows in each data set and some additional details at the end of this section. \n",
    "\n",
    "1. **[515k Hotel Reviews in Europe](https://www.kaggle.com/jiashenliu/515k-hotel-reviews-data-in-europe \"Kaggle 515k reviews\")**: \n",
    "This data set by Jiashen Liu from 2017 contains European hotel review data scraped from Booking.com. The data set was originally created mainly for sentiment analysis, and contains fields such as positive and negative words in the reviews. Other interesting fields in terms of this project are GPS (which I will use to join to data sets 2 and 3 below), address and reviewers' nationality (which I will use to join some additional data sets -- data sets 4-16). \n",
    "\n",
    "The complete list of fields (taken mostly from the Kaggle page, but with some additional explanation by me) is given below:\n",
    "* Hotel_Address: Address of hotel. This is not separated into country, city etc, and is sometimes random\n",
    "* Review_Date: Date when reviewer posted the corresponding review.\n",
    "* Average_Score: Average Score of the hotel, calculated based on the latest comment in the last year.\n",
    "* Hotel_Name: Name of Hotel\n",
    "* Reviewer_Nationality: Nationality of Reviewer\n",
    "* Negative_Review: Negative part of the review. If the reviewer does not give a negative review, then the value should be 'No Negative'.\n",
    "* ReviewTotalNegativeWordCounts: Total number of words in the negative review.\n",
    "* Positive_Review: Positive part of the review. If the reviewer does not give a negative review, then the value should be: 'No Positive'.\n",
    "* ReviewTotalPositiveWordCounts: Total number of words in the positive review.\n",
    "* Reviewer_Score: Score the reviewer has given to the hotel, based on his/her experience\n",
    "* TotalNumberofReviewsReviewerHasGiven: Number of Reviews the reviewers has given in the past.\n",
    "* TotalNumberof_Reviews: Total number of valid reviews the hotel has.\n",
    "* Tags: Tags reviewer gave the hotel.\n",
    "* dayssincereview: Duration between the review date and scrape date. This is a pointless field for my project.\n",
    "* AdditionalNumberof_Scoring: The data set's author has added the number of ratings for the hotels here, i.e., the number of users who have rated a hotel but not actually written a review.\n",
    "* lat: Latitude of the hotel\n",
    "* lng: longtitude of the hotel\n",
    "\n",
    "2. **[Google Local Data Set](https://cseweb.ucsd.edu/~jmcauley/datasets.html#google_local \"Google Local data set\")**: \n",
    "This is a data set from Julian Macauley and others which contains data from Google Maps. The authors released the data set as part of 2 papers: ['Translation-based recommendation'](http://cseweb.ucsd.edu/~jmcauley/pdfs/recsys17.pdf) and ['Translation-based Factorization Machines for SequentialRecommendation'](http://cseweb.ucsd.edu/~jmcauley/pdfs/recsys18a.pdf). While the first data set I chose was meant for sentiment analysis, this data set is meant for recommender systems.\n",
    "\n",
    "The Google Local data set is actually made up of 3 different data sets: one about businesses on Google Maps, one about Google Maps users (local guides), and one containing Google Maps reviews data. For this project, I use only the Google Maps businesses data, and only I have explained only this data set below.\n",
    "The Google Local Businesses data contains data which has been added by the businesses themselves or the Google Local Guide community on Google Maps. \n",
    "\n",
    "The data is in JSON format (it's actually in invalid json format, more about that in the next section). A sample record is given below: \n",
    "\n",
    "{\"name\": \"Portofino\", \"price\": null, \"address\": [\"\\u0443\\u043b. \\u0422\\u0443\\u0442\\u0430\\u0435\\u0432\\u0430, 1\", \"Nazran, Ingushetia, Russia\", \"366720\"], \"hours\": [[\"Monday\", [[\"9:30 am--9:00 pm\"]]], [\"Tuesday\", [[\"9:30 am--9:00 pm\"]]], [\"Wednesday\", [[\"9:30 am--9:00 pm\"]], 1], [\"Thursday\", [[\"9:30 am--9:00 pm\"]]], [\"Friday\", [[\"9:30 am--9:00 pm\"]]], [\"Saturday\", [[\"9:30 am--9:00 pm\"]]], [\"Sunday\", [[\"9:30 am--9:00 pm\"]]]], \"phone\": \"8 (963) 173-38-38\", \"closed\": false, \"gPlusPlaceId\": \"109810290098030327104\", \"gps\": [43.22776, 44.762726]}\n",
    "\n",
    "The fields of the data set are:\n",
    "* name: the name of the hotel\n",
    "* price: an indicator of how expensive the hotel is. Possible values are '$', '$$', '$$$'. The authors of this data set are American, so this is in dollars rather than euros or any other currency. Null if not present\n",
    "* address: this is a JSON array with the country, state, city and pincode. However, this data (like in the first data set) seems to be very unstructured. The city might come after the country at times, the pincode may come at the end or the middle, and very annoyingly, the authors have decided to not include a country for American addresses. This project only uses European address.\n",
    "* hours: a nested JSON array with timings for each day in turn. Null if not present\n",
    "* phone: phone number, if present\n",
    "* closed: True or False, I don't use this field in my project.\n",
    "* gPlusPlaceId: An id for the place on Google Maps/Google Plus\n",
    "* gps: A JSON array containing the latitude and longitude. Note that the same place has different gps coordinates in different data sets. The 515k reviews and Google Local data set will have different coordinates for the same place, and we therefore need a fuzzy join of some kind.\n",
    "\n",
    "3. **[Airport codes Data Set](https://datahub.io/core/airport-codes#data \"Airport codes data set\")**:\n",
    "\n",
    "This data set contains information about airports. Most of the fields are self-explanatory\n",
    "\n",
    "* ident: identifier\n",
    "* type: type of airport\n",
    "* name: airport name\n",
    "* elevation_ft: elevation of airport in feet\n",
    "* continent: continent where airport is situated\n",
    "* iso_conuntry: iso country code where airport is situated.\n",
    "* iso_region_code: region where airport is located.\n",
    "* municipality\n",
    "* gps code\n",
    "* iata_code\n",
    "* local_code\n",
    "* coordinates: latitude, longitude pair.\n",
    "\n",
    "Details about the 3 main data which have already been explained are given in a table below.\n",
    "\n",
    "\n",
    "| Data set        | Num_rows           |   Data Source | Comment | Description | Year |\n",
    "| ------------- |:-------------:| -----:| ----------:| ----------------------:| ------:|\n",
    "|[515k Hotel Reviews in Europe](https://www.kaggle.com/jiashenliu/515k-hotel-reviews-data-in-europe \"Kaggle 515k reviews\")|515,738|Kaggle| Data exploration done in Explore_reviews.ipynb | Hotel review data scraped from Booking.com|2017 |\n",
    "|[Google Local Businesses Data Set](https://cseweb.ucsd.edu/~jmcauley/datasets.html#google_local \"Google Local data set\")|3,114,353 (464,906 for Spain, France, UK, Italy, Austria, Netherlands)|J.McAuley and others| Data exploration done in Explore_google_places.ipynb | Data about businesses in Google Maps |2018 |\n",
    "|[Airport codes Data Set](https://datahub.io/core/airport-codes#data \"Airport codes data set\")|55,075|Datahub.io| Data exploration done in Explore_airport_codes.ipynb | Simple airport codes data |2018 |\n",
    "\n",
    "4. **Additional data sets(data sets 4 through 17)**:\n",
    "\n",
    "Details about these data sets are given in the form of a table below. Most of these data sets have a simple structure. They only have one or two columns, which can be identified from the name of the data set. I have removed any additional columns which aren't required They are pretty small too. I have combined these disparate data sets in a separate Jupyter notebbok: **CombineAdditionalCountryData.ipynb**. These data set isn't very large, but I believe that they help to answer some interesting questions about reviews with relation to the reviwers' nationality. The CombineAdditionalCountryData notebook also includes the rationale for including these data sets in my data model.  A short description of the data sets is given below. \n",
    "\n",
    "| Data set        | Num_rows           |   Data Source | Comment | Description | Year |\n",
    "| ------------- |:-------------:| -----:| ----------:| ----------------------:| ------:|\n",
    "| [Country List ISO](https://datahub.io/core/country-list#resource-data)     | 249 | datahub.io| -| Contains a list of countries along with their 2-digit ISO code.. |- |\n",
    "| [Tourist-Visitors Arrival and Expenditure](http://data.un.org/)     | 2246 (whittled down to 220) | UNWTO | Found under 'Tourism and transport' after following the link | Data related to different countries' spending on tourism and the no. of inbound visitors/tourists |2018 |\n",
    "| [Exchange rates](http://data.un.org/)     | 3408 (whittled down to 234) | IMF | Found under 'Finance' after following the link | Data related to exchange rates at the end of 2018 | 2018 |\n",
    "| [GNI Per Capita](http://hdr.undp.org/en/data)     | 191 | UNDP | Found under dimension='Income/composition of resources' after following the link | Gives the Gross National Income  in dollars(2011 PPP) | 2018 |\n",
    "| [GDP Per Capita](http://hdr.undp.org/en/data)     | 192 (whittled down to 220) | UNDP | Found under dimension='Income/composition of resources' after following the link | Gives the Gross Domestic Product in dollars (2011 PPP) | 2018 |\n",
    "| [Internet Users As Percentage of Population](http://hdr.undp.org/en/data)     | 195 | UNDP | Found under dimension='Mobility and Communiucation' after following the link | Gives the percentage of the total population who are internet users | 2018 |\n",
    "| [Mobile Phone Subscriptions](http://hdr.undp.org/en/data)     | 195 | UNDP | Found under dimension='Income/composition of resources' after following the link | Gives the mobile phone subscriptions per 100 people (>100: people have >1 mobile connection on average) | 2018 |\n",
    "| [Net Migration Rate](http://hdr.undp.org/en/data)     | 191 | UNDP | Found under dimension='Income/composition of resources' after following the link | Gives the net migration rate (per 1000 people) | 2020 |\n",
    "| [Population](http://hdr.undp.org/en/data)     | 195 | UNDP | Found under dimension='Demography' after following the link | Gives the total population (in millions) | 2018 |\n",
    "| [Urban Population Percentage](http://hdr.undp.org/en/data)     | 195 | UNDP | Found under dimension='Human Development Index' after following the link | Gives the urban population as a percentage of the total population | 2018 |\n",
    "| [Human Development Index (HDI)](http://hdr.undp.org/en/data)     | 195 | UNDP | Found under dimension='Income/composition of resources' after following the link | Gives the Human Development Index and the corresponding rank in 2018 | 2018 |\n",
    "| [2020_Country_and_Territory_Ratings_and_Statuses_FIW2020](https://freedomhouse.org/report/freedom-world)     | 205 (whittled  down to 195) | Freedom House | I have included only the latest data, not all the data from 1973-2020| Gives 2 indicators of freedom: Political Rights and Civil Liberties, both of which are scored on a 1-7 scale. A column called Status has values corresponding to 'Free', 'Not Free', 'Partially Free'. | 2020 |\n",
    "| [2020_List_of_Electoral_Democracies_FIW_2020](https://freedomhouse.org/report/freedom-world)     | 195 | Freedom House | I have included only the latest data| Gives a list of countries and whether or not they are democracies: Yes or No | 2020 |\n",
    "| [human-rights-score-vs-political-regime-type](https://ourworldindata.org/democracy)     | 35333 (whittled down to 196) | Our World in Data| -| Gives a list of countries along with their  political regime type (score) and human rights protection score. The political regime score ranges from -10 (autocracy) to +10 (full democracy). The Human Rights Scores (the higher the better) were first developed by Schnakenberg and Farris (2014) and subsequently updated by Farris (2019). |2015 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Exploring and cleaning the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc. Document steps necessary to clean the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "I have done the cleaning of the individual data sets in separate notebooks. Below, I describe what the problems are, what my solution is, and in which notebook I actually do the cleaning. \n",
    "\n",
    "#### 515k reviews data: \n",
    "* There are a few string columns with value = ''.  I replace these with null. There are also leading and trailing spaces in string columns. Both these cleaning tasks are performed in **Step 30 of the PrepareDataSetsForS3 notebook**. Also, see the exploraratory analysis notebook *DataExploration/Explore515kReviews.ipynb* for additional details. \n",
    "*  The latitude and longitude are missing in a few rows. Instead, they have a value 'NA'. As these are important columns for our fuzzy join, we need to insert correct latitude and longitude values. 2 possibilities are to get the corresponding values from the Google Local Data (see **Step 8 and 27 of the PrepareDataSetsForS3 notebook**).  \n",
    "* The hotel name and hotel address in this data set do not render Latin (unicode) characters correctly -- umlauts and accent graves, for example. I get the hotel name from the Google Local data, if the hotel is available there. See **Step 13 of the PrepareDataSetsForS3 notebook**.\n",
    "* The date is a string in M/dd/yyyy format, which is definitely not what we want. I convert it to a date with the format yyyy-mm-dd in **Step 31 of the PrepareDataSetsForS3 notebook**\n",
    "* Finally, I drop and rename columns as necessary. This is done in various steps in the **PrepareDataSetsForS3 notebook**.\n",
    "\n",
    "#### Google Local Data: \n",
    "* The Google Local businesses data was very messy. It it supposed to be in JSON lines format, but the format is actually invalid JSON. Also, it cannot be read by Spark, which gives an error (corrupted record). There are plenty of issues which need to be fixed before it can be read successfully by Spark. These issues are addressed in the **CleanGooglePlaces notebook**. This notebook is the first notebook which needs to be executed while starting the project, as the PrepareDataSetsForS3 uses its results. The next 4 points explain the issues addressed in this notebook. \n",
    "* The json had single quotes instead of double quotes, which are the JSON standard. Possible trailing commas also need to be fixed. \n",
    "* There are nested JSON arrays which need to be flattened to be read in to Spark. \n",
    "* I performed these tasks using the JSON library and a for loop (there is no other way). See **CleanGooglePlaces notebook** for all the details. \n",
    "* I also restrict the data to only the 6 European countries which are present in the 515k reviews data. See also the exploratory analysis notebook *DataExploration/ExploreGooglePlacesSample.ipynb* (which details the cleaning steps in detail for the first 13 lines of the data set) \n",
    "* Another issue is that the latitude and longitude values are sometimes outside the valid range [-90,90]. These seem to be off by a factor of 1 million! I correct the latitude and longitude values for hotels data by joining with the 515k reviews data and getting the latitude and longitude from there (I do this only for the matching hotels data and not for all the businesses in Google Local). See **Step 9 of the PrepareDataSetsForS3 notebook**.\n",
    "* Finally, I drop and rename columns as necessary. This is done in various steps in the **PrepareDataSetsForS3 notebook**.\n",
    "\n",
    "#### Airport codes Data: \n",
    "* There are some spam rows, which need to be deleted. See **Step 19 of the PrepareDataSetsForS3 notebook** and the exploratory analysis notebook *DataExploration/ExploreAirportCodes.ipynb notebook*.\n",
    "* There is no country name column missing, I get this from an ISO codes to country mapping data set, and add it in. This is done in **Step 20 of the PrepareDataSetsForS3 notebook**. \n",
    "* Finally, I drop and rename columns as necessary. This is done in various steps in the **PrepareDataSetsForS3 notebook**.\n",
    "#### Additional Country data sets: \n",
    "* These data sets often have different types of values instead of NULL. For instance, some data sets obtained from UNDP (see **Part 2 of the CombineAdditionalCountryData notebook**  have the value '..'. I replace these values with null. The operations on these 14 data sets are shown in **CombineAdditionalCountryData**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
